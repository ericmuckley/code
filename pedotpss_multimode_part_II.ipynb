{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pedotpss_multimode_part_II.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericmuckley/code/blob/master/pedotpss_multimode_part_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fimy_FdgiDbt",
        "colab_type": "text"
      },
      "source": [
        "# Import the data\n",
        "\n",
        "This section will use the data formatted in Part I and pull it from Github\n",
        "by first cloning the github repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onjj-fNBiYLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYgcmzQbiYRl",
        "colab_type": "code",
        "outputId": "0b7fbd65-5c66-4a91-eb33-2cb995856a55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# clone the entire github repository\n",
        "%cd /content/\n",
        "!rm -rf cloned-data\n",
        "!git clone -l -s git://github.com/ericmuckley/datasets.git cloned-data\n",
        "\n",
        "# navigate to the repo\n",
        "%cd cloned-data\n",
        "print('-------------------------- Files in repo: --------------------------')\n",
        "!ls\n",
        "\n",
        "def open_pickle(filename):\n",
        "    '''\n",
        "    Opens serialized Python pickle file as a dictionary.\n",
        "    Filename should be something like 'saved_data.pkl'.\n",
        "    '''\n",
        "    with open(filename, 'rb') as handle:\n",
        "        dic = pickle.load(handle)\n",
        "    return dic\n",
        "\n",
        "# open the file we want\n",
        "dic = open_pickle('pp_multimode.pkl')\n",
        "# get out of the data folder\n",
        "%cd /content/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'cloned-data'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 67, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 67 (delta 27), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (67/67), 6.44 MiB | 12.50 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n",
            "/content/cloned-data\n",
            "-------------------------- Files in repo: --------------------------\n",
            "hops_enose_response.csv        milk__FTIR.csv\tpedotpss_multimode.xlsx\n",
            "milk_drop_QCM_aug12_2019.xlsx  milk__RAMAN.csv\tpp_multimode.pkl\n",
            "milk_FTIR_by_hour.csv\t       milk__UVVIS.csv\tREADME.md\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l39D6A-9iYPP",
        "colab_type": "code",
        "outputId": "84c6f20d-46b9-417a-f121-dd3e09473ce4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# examine the data\n",
        "for key in dic:\n",
        "    print('%s: %s, shape: %s' %(\n",
        "        key, str(type(dic[key])), str(np.shape(dic[key]))))\n",
        "print('\\nscalars columns:')\n",
        "print(np.array(list(dic['scalars'])))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spe_n: <class 'pandas.core.frame.DataFrame'>, shape: (200, 190)\n",
            "spe_k: <class 'pandas.core.frame.DataFrame'>, shape: (200, 190)\n",
            "spe_psi: <class 'pandas.core.frame.DataFrame'>, shape: (200, 190)\n",
            "spe_delta: <class 'pandas.core.frame.DataFrame'>, shape: (200, 190)\n",
            "eis_z: <class 'pandas.core.frame.DataFrame'>, shape: (200, 190)\n",
            "eis_phase: <class 'pandas.core.frame.DataFrame'>, shape: (200, 190)\n",
            "eis_rez: <class 'pandas.core.frame.DataFrame'>, shape: (200, 190)\n",
            "eis_imz: <class 'pandas.core.frame.DataFrame'>, shape: (200, 190)\n",
            "iv: <class 'pandas.core.frame.DataFrame'>, shape: (200, 190)\n",
            "scalars: <class 'pandas.core.frame.DataFrame'>, shape: (189, 27)\n",
            "\n",
            "scalars columns:\n",
            "['rh' 'df1' 'dd1' 'df7' 'dd7' 'df11' 'dd11' 'dmu1' 'deta1' 'dmu7' 'deta7'\n",
            " 'dmu11' 'deta11' 'cvarea10' 'cvarea50' 'cvarea100' 'cvarea300' 'cvmax10'\n",
            " 'cvmax50' 'cvmax100' 'cvmax300' 'ivmaxcurr' 'ivmincurr' 'z1hz' 'phi1hz'\n",
            " 'n600' 'k600']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpX6OyCcN-1E",
        "colab_type": "text"
      },
      "source": [
        "#  Creating regression models using scikit-learn\n",
        "\n",
        "Now that the data is clean and formatted, we can prepare it for use in building\n",
        "a predictive machine learning model. For this model we will look at just \n",
        "the scalar data.\n",
        "\n",
        "This section provides functions for splitting and examining the datasets,\n",
        "fitting the model, and testing the model.\n",
        "\n",
        "## Standardize data and split into training & testing sets\n",
        "\n",
        "Before fitting the model, we standardize (scale and normalize) the data.\n",
        "Then we can split the standardized data into training and testing sets, using\n",
        "specified row indices to designate the samples in each set.\n",
        "\n",
        "The data is split into 4 sets and input sets are scaled:\n",
        "\n",
        "**in_train**: input features for training\n",
        "\n",
        "**out_train**: output/target features for training\n",
        "\n",
        "**in_test**: input features for testing\n",
        "\n",
        "**out_test**: output/target features for testing\n",
        "\n",
        "We use our own custom function for this so we can perform it again later.\n",
        "The function will take the dataset as an input in the form of a Pandas \n",
        "DataFrame. The funtion will also need the names of input and target columns,\n",
        "and lists of which samples (row indices) we will use for training and testing.\n",
        "Then we write a custom function to fit the model and make predictions from\n",
        "the trained model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKBFDE2ywLSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we import the necessary libraries again here so we can start from this\n",
        "# section in the future\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy.polynomial.polynomial as poly\n",
        "\n",
        "def plot_setup(labels=['X', 'Y'], size=16, setlimits=False, limits=[0,1,0,1]):\n",
        "    # This can be called with Matplotlib for setting axes labels,\n",
        "    # setting axes ranges, and setting the font size of plot labels.\n",
        "    # Should be called between plt.plot() and plt.show() commands.\n",
        "    plt.rcParams['xtick.labelsize'] = size \n",
        "    plt.rcParams['ytick.labelsize'] = size\n",
        "    plt.xlabel(str(labels[0]), fontsize=size)\n",
        "    plt.ylabel(str(labels[1]), fontsize=size)\n",
        "    if setlimits:\n",
        "        plt.xlim((limits[0], limits[1]))\n",
        "        plt.ylim((limits[2], limits[3]))\n",
        "\n",
        "\n",
        "def find_key_of_value(value, dictionary):\n",
        "    '''\n",
        "    Checks if a value exists inside any keys of a dictionary.\n",
        "    Returns a list of keys in which the value resides, and a list of the\n",
        "    other values in the keys.\n",
        "    '''\n",
        "    keys = []\n",
        "    other_values = []\n",
        "    for key in dictionary:\n",
        "        # check if value resides in the key \n",
        "        if value in dictionary[key]:\n",
        "            keys.append(key)\n",
        "            # find other values inside the key\n",
        "            for val in dictionary[key]:\n",
        "                if val != value:\n",
        "                    other_values.append(val)\n",
        "    return keys[0], other_values\n",
        "\n",
        "\n",
        "\n",
        "def format_for_ml(df0, target_feature, train_samples,\n",
        "                  drop_features=None, scaler=StandardScaler()):\n",
        "    '''\n",
        "    Splits a Pandas DataFrame (df0) into different arrays for \n",
        "    use in ML models. The df is split into input and output (target)\n",
        "    feature by column name. Columns which are not target_feature\n",
        "    are used as input features. \n",
        "\n",
        "    The df is split into training and testing arrays according\n",
        "    to the list of indices (rows) designated by train_samples.\n",
        "    The rows which are not used for training are used as test_samples.\n",
        "    Features can be dropped by using a list of column names in the\n",
        "    drop_features agument.\n",
        "    \n",
        "    Use a scaler to create scaled arrays for use in the ML model.\n",
        "    The output is a dictionary:\n",
        "    in_train: input features for training\n",
        "    out_train: target feature for training\n",
        "    in_test: input features for testing\n",
        "    out_test: target feature for testing\n",
        "    in_train_s: scaled input features for training\n",
        "    out_train_s: scaled target feature for training\n",
        "    in_test_s: scaled input features for testing\n",
        "    out_test_s: scaled target feature for testing\n",
        "    '''\n",
        "    df = df0.copy()\n",
        "    # drop unwanted features\n",
        "    if drop_features is not None:\n",
        "        df.drop(drop_features, inplace=True, axis=1)\n",
        "\n",
        "    # for input features, use all columns that are not used as target_feature\n",
        "    input_features = [col for col in list(df) if col != target_feature]\n",
        "    # for test samples, use all samples that are not used as training samples\n",
        "    test_samples = np.setdiff1d(df.index.values, train_samples)\n",
        "\n",
        "    # create datasets\n",
        "    in_train = df[input_features].iloc[train_samples]\n",
        "    out_train = df[target_feature].iloc[train_samples]\n",
        "    in_test = df[input_features].iloc[test_samples]\n",
        "    out_test = df[target_feature].iloc[test_samples]\n",
        "\n",
        "    # fit the standardization scaler\n",
        "    scaler_in = scaler\n",
        "    scaler_in.fit(df[input_features].values)\n",
        "    \n",
        "    # standardize the input datasets using the scaler\n",
        "    in_train_s = scaler.transform(in_train.values)\n",
        "    in_test_s = scaler.transform(in_test.values)\n",
        "\n",
        "    # store datasets in a dictionary\n",
        "    ds = {\n",
        "        'scaler': scaler,\n",
        "        'in_train': in_train,\n",
        "        'out_train': out_train,\n",
        "        'in_test': in_test,\n",
        "        'out_test': out_test,\n",
        "        'in_train_s': in_train_s,\n",
        "        'in_test_s': in_test_s,\n",
        "        'target_feature': target_feature}\n",
        "    return ds\n",
        "\n",
        "\n",
        "def examine_ml_datasets(ds):\n",
        "    # examine the datasets produced by the \"format_for_ml\" function\n",
        "    # loop over each dataset in the dictionary\n",
        "    for d in ds:\n",
        "        if isinstance(ds[d], pd.DataFrame):\n",
        "            print('%s: shape=%s, columns=%s' %(\n",
        "                str(d), str(ds[d].shape), str(list(ds[d]))))\n",
        "        if isinstance(ds[d], np.ndarray):\n",
        "            print('%s: shape=%s' %(str(d), str(ds[d].shape)))\n",
        "        if isinstance(ds[d], str):\n",
        "            print('%s: %s' %(d, ds[d]))\n",
        "\n",
        "def deploy_model(model, dataset, ind_var_name='rh'):\n",
        "    '''\n",
        "    Fits a regression model, makes predictions with the model, and assesses\n",
        "    quality of the model. For \"model\" use a model (e.g. sklearn SVR()),\n",
        "    and for \"ds\" use dataset from the output of the function \"format_for_ml\"\n",
        "    '''\n",
        "    # fit the model using standardized input and output training sets\n",
        "    model.fit(ds['in_train_s'], ds['out_train'])\n",
        "    # make predictions based on the training data\n",
        "    train_prediction = model.predict(ds['in_train_s'])\n",
        "    # make predictions based on the testing data\n",
        "    test_prediction = model.predict(ds['in_test_s'])\n",
        "    # assess performance of the model\n",
        "    model_score = model.score(ds['in_test_s'], ds['out_test'])\n",
        "    raw_error = np.abs(test_prediction - ds['out_test'])\n",
        "    avg_percent_error = np.mean(np.abs(raw_error/dataset['out_test']))*100\n",
        "    # buld a dictionary to summarize model results\n",
        "    model_results = {\n",
        "        'model': model,\n",
        "        'raw_error': raw_error,\n",
        "        'model_score': model_score,\n",
        "        'test_prediction': test_prediction,\n",
        "        'train_prediction': train_prediction,\n",
        "        'avg_percent_error': avg_percent_error}\n",
        "    return model_results\n",
        "\n",
        "\n",
        "def plot_reg_results(ds, model_results, ind_var_name=None,\n",
        "                     model_name='', plot_scatter=False):\n",
        "    '''\n",
        "    Plots results of regression model fitting and testing using a model\n",
        "    results dictionary (model_results) and the model dataset (ds).\n",
        "    The plots are referenced to ind_var_name, which is the title of the\n",
        "    column to use for a x-axis in plots (i.e. time, RH, temp.). If\n",
        "    ind_var_name=None, the x-axis values are just indices of the samples.\n",
        "    The model_name string argument can be added to specify the model name\n",
        "    on the title of the plots.\n",
        "    The plot_scatter argument adds a scatter plot of measured vs\n",
        "    predicted samples. \n",
        "    '''\n",
        "    if ind_var_name is None:\n",
        "        ind_var = np.arange(len(ds['in_test']))\n",
        "    else:\n",
        "        ind_var = ds['in_test'][ind_var_name]\n",
        "    # plot predicted data\n",
        "    plt.plot(ind_var,\n",
        "                model_results['test_prediction'],\n",
        "                label='prediction', c='g')\n",
        "    # plot target data\n",
        "    plt.scatter(ind_var, ds['out_test'],\n",
        "                label='target samples', c='b', s=4)\n",
        "    # plot error between target and prediction\n",
        "    # plt.plot(ind_var, model_results['raw_error'], label='error', c='r')\n",
        "    # plot training input data\n",
        "    plt.scatter(ds['in_train'][ind_var_name],\n",
        "            ds['out_train'],\n",
        "            label='training samples', c='c', s=4)\n",
        "    plot_setup(labels=[ind_var_name,\n",
        "                        ds['target_feature'].upper()])\n",
        "    plt.title(model_name + ' prediction of '+ds['target_feature'].upper(),\n",
        "                fontsize=16)\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.show()\n",
        "    # scatter plot of predicted points and expected output points\n",
        "    if plot_scatter:\n",
        "        plt.scatter(model_results['train_prediction'],\n",
        "                    ds['out_train'],\n",
        "                    c='k', s=3, label='train samples')\n",
        "        plt.scatter(model_results['test_prediction'],\n",
        "                    ds['out_test'],\n",
        "                    c='r', s=3, label='test samples')\n",
        "        plot_setup(labels=['Measured value', 'Predicted value'])\n",
        "        plt.title(model_name + ' predicted vs. measured samples',\n",
        "                    fontsize=16)\n",
        "        plt.legend(fontsize=14)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8yl9Oi6Flun",
        "colab_type": "text"
      },
      "source": [
        "## Identify regression models to use\n",
        "\n",
        "Now we create a list of models to fit so we can compare the performance of\n",
        "multiple models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR0uQMU39-8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.linear_model as lm\n",
        "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.svm import LinearSVR, NuSVR, SVR\n",
        "\n",
        "def compare_fits(\n",
        "    x, y, x_new=None, fits=['poly1', 'poly2', 'poly3', 'spline'], s=5):\n",
        "    '''\n",
        "    Compares least squares fits to x and y ordered pairs.\n",
        "    Performs fits at x_new points. The fits argument is a\n",
        "    list of strings which indicate which fits to compare.\n",
        "    fits = [\n",
        "        'poly1' = degree 1 polynomial (linear) fit \n",
        "        'poly2' = degree 2 polynomial (quadratic) fit\n",
        "        'poly3' = degree 3 polynomial fit\n",
        "        'spline' = B-spline fit\n",
        "            ]\n",
        "    s = smoothing factor for the spline fit\n",
        "    '''\n",
        "    # if new x values are not passed, use original x values\n",
        "    if x_new is None:\n",
        "        x_new = x\n",
        "    # create dictionary to hold fits\n",
        "    fit_dic = {}\n",
        "    # perform fits\n",
        "    if 'poly1' in fits:\n",
        "        poly1coefs = poly.polyfit(x, y, 1)\n",
        "        poly1fit = poly.polyval(x_new, poly1coefs)\n",
        "        fit_dic['poly1'] = poly1fit\n",
        "    if 'poly2' in fits:\n",
        "        poly2coefs = poly.polyfit(x, y, 2)\n",
        "        poly2fit = poly.polyval(x_new, poly2coefs)\n",
        "        fit_dic['poly2'] = poly2fit\n",
        "    if 'poly3' in fits:\n",
        "        poly3coefs = poly.polyfit(x, y, 3)\n",
        "        poly3fit = poly.polyval(x_new, poly3coefs)\n",
        "        fit_dic['poly3'] = poly3fit\n",
        "    if 'spline' in fits:\n",
        "        spline_params = splrep(x, y, s=s, k=3)\n",
        "        splinefit = splev(x_new, spline_params)\n",
        "        fit_dic['spline'] = splinefit\n",
        "    return fit_dic\n",
        "\n",
        "\n",
        "modeldict = {\n",
        "    'ardregression': lm.ARDRegression(),\n",
        "    'bayesianridge': lm.BayesianRidge(),\n",
        "    'elasticnet': lm.ElasticNet(),\n",
        "    'elasticnetcv': lm.ElasticNetCV(),\n",
        "    'huberregression': lm.HuberRegressor(),\n",
        "    'lars': lm.Lars(),\n",
        "    'larscv': lm.LarsCV(),\n",
        "    'lasso': lm.Lasso(),\n",
        "    'lassocv': lm.LassoCV(),\n",
        "    'lassolars': lm.LassoLars(),\n",
        "    'lassolarscv': lm.LassoLarsCV(),\n",
        "    'lassolarsic': lm.LassoLarsIC(),\n",
        "    'linearregression': lm.LinearRegression(),\n",
        "    'orthogonalmatchingpursuit': lm.OrthogonalMatchingPursuit(),\n",
        "    'orthogonalmatchingpursuitcv': lm.OrthogonalMatchingPursuitCV(),\n",
        "    'passiveagressiveregressor': lm.PassiveAggressiveRegressor(),\n",
        "    'ridge': lm.Ridge(),\n",
        "    'ridgecv': lm.RidgeCV(),\n",
        "    'sgdregressor': lm.SGDRegressor(),\n",
        "    'theilsenregressor': lm.TheilSenRegressor(),\n",
        "    'decisiontreeregressor': DecisionTreeRegressor(),\n",
        "    'randomforestregressor': RandomForestRegressor(),\n",
        "    'adaboostregressor': AdaBoostRegressor(),\n",
        "    'baggingregressor': BaggingRegressor(),\n",
        "    'extratreeregressor': ExtraTreeRegressor(),\n",
        "    'linearsvr': LinearSVR(),\n",
        "    'nusvr': NuSVR(),\n",
        "    'svr': SVR()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvBC49XVJrLr",
        "colab_type": "text"
      },
      "source": [
        "### Examine our dataset features and regression models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO0LuqgjG61M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce04ffa1-4487-4648-bc0c-c45887387512"
      },
      "source": [
        "# print all the features we have in our dataset\n",
        "print('%s total features:' %str(len(list(dic['scalars']))))\n",
        "print(str(list(dic['scalars'])))\n",
        "\n",
        "# create dictionary of all features organized by measurement type\n",
        "feature_dict = {\n",
        "    'qcm': ['df1', 'dd1', 'df7', 'dd7', 'df11', 'dd11', 'dmu1',\n",
        "            'deta1', 'dmu7', 'deta7', 'dmu11', 'deta11'],\n",
        "    'dc': ['cvarea10', 'cvarea50', 'cvarea100', 'cvarea300', 'cvmax10',\n",
        "               'cvmax50', 'cvmax100', 'cvmax300', 'ivmaxcurr', 'ivmincurr'],\n",
        "    'ac': ['z1hz', 'phi1hz'],\n",
        "    'optical': ['n600', 'k600']}\n",
        "\n",
        "for key in feature_dict:\n",
        "    print('%i %s features: , %s' %(\n",
        "        len(feature_dict[key]), key.upper(), str(feature_dict[key])))\n",
        "\n",
        "for model_i, model in enumerate(modeldict):\n",
        "    print('\\nModel # %i: ' %(model_i+1))\n",
        "    print(modeldict[model])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27 total features:\n",
            "['rh', 'df1', 'dd1', 'df7', 'dd7', 'df11', 'dd11', 'dmu1', 'deta1', 'dmu7', 'deta7', 'dmu11', 'deta11', 'cvarea10', 'cvarea50', 'cvarea100', 'cvarea300', 'cvmax10', 'cvmax50', 'cvmax100', 'cvmax300', 'ivmaxcurr', 'ivmincurr', 'z1hz', 'phi1hz', 'n600', 'k600']\n",
            "12 QCM features: , ['df1', 'dd1', 'df7', 'dd7', 'df11', 'dd11', 'dmu1', 'deta1', 'dmu7', 'deta7', 'dmu11', 'deta11']\n",
            "10 DC features: , ['cvarea10', 'cvarea50', 'cvarea100', 'cvarea300', 'cvmax10', 'cvmax50', 'cvmax100', 'cvmax300', 'ivmaxcurr', 'ivmincurr']\n",
            "2 AC features: , ['z1hz', 'phi1hz']\n",
            "2 OPTICAL features: , ['n600', 'k600']\n",
            "\n",
            "Model # 1: \n",
            "ARDRegression(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
            "              fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
            "              normalize=False, threshold_lambda=10000.0, tol=0.001,\n",
            "              verbose=False)\n",
            "\n",
            "Model # 2: \n",
            "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
            "              fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
            "              normalize=False, tol=0.001, verbose=False)\n",
            "\n",
            "Model # 3: \n",
            "ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
            "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
            "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
            "\n",
            "Model # 4: \n",
            "ElasticNetCV(alphas=None, copy_X=True, cv='warn', eps=0.001, fit_intercept=True,\n",
            "             l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=None,\n",
            "             normalize=False, positive=False, precompute='auto',\n",
            "             random_state=None, selection='cyclic', tol=0.0001, verbose=0)\n",
            "\n",
            "Model # 5: \n",
            "HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,\n",
            "               tol=1e-05, warm_start=False)\n",
            "\n",
            "Model # 6: \n",
            "Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,\n",
            "     n_nonzero_coefs=500, normalize=True, positive=False, precompute='auto',\n",
            "     verbose=False)\n",
            "\n",
            "Model # 7: \n",
            "LarsCV(copy_X=True, cv='warn', eps=2.220446049250313e-16, fit_intercept=True,\n",
            "       max_iter=500, max_n_alphas=1000, n_jobs=None, normalize=True,\n",
            "       positive=False, precompute='auto', verbose=False)\n",
            "\n",
            "Model # 8: \n",
            "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
            "      normalize=False, positive=False, precompute=False, random_state=None,\n",
            "      selection='cyclic', tol=0.0001, warm_start=False)\n",
            "\n",
            "Model # 9: \n",
            "LassoCV(alphas=None, copy_X=True, cv='warn', eps=0.001, fit_intercept=True,\n",
            "        max_iter=1000, n_alphas=100, n_jobs=None, normalize=False,\n",
            "        positive=False, precompute='auto', random_state=None,\n",
            "        selection='cyclic', tol=0.0001, verbose=False)\n",
            "\n",
            "Model # 10: \n",
            "LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,\n",
            "          fit_path=True, max_iter=500, normalize=True, positive=False,\n",
            "          precompute='auto', verbose=False)\n",
            "\n",
            "Model # 11: \n",
            "LassoLarsCV(copy_X=True, cv='warn', eps=2.220446049250313e-16,\n",
            "            fit_intercept=True, max_iter=500, max_n_alphas=1000, n_jobs=None,\n",
            "            normalize=True, positive=False, precompute='auto', verbose=False)\n",
            "\n",
            "Model # 12: \n",
            "LassoLarsIC(copy_X=True, criterion='aic', eps=2.220446049250313e-16,\n",
            "            fit_intercept=True, max_iter=500, normalize=True, positive=False,\n",
            "            precompute='auto', verbose=False)\n",
            "\n",
            "Model # 13: \n",
            "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
            "\n",
            "Model # 14: \n",
            "OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,\n",
            "                          normalize=True, precompute='auto', tol=None)\n",
            "\n",
            "Model # 15: \n",
            "OrthogonalMatchingPursuitCV(copy=True, cv='warn', fit_intercept=True,\n",
            "                            max_iter=None, n_jobs=None, normalize=True,\n",
            "                            verbose=False)\n",
            "\n",
            "Model # 16: \n",
            "PassiveAggressiveRegressor(C=1.0, average=False, early_stopping=False,\n",
            "                           epsilon=0.1, fit_intercept=True,\n",
            "                           loss='epsilon_insensitive', max_iter=1000,\n",
            "                           n_iter_no_change=5, random_state=None, shuffle=True,\n",
            "                           tol=0.001, validation_fraction=0.1, verbose=0,\n",
            "                           warm_start=False)\n",
            "\n",
            "Model # 17: \n",
            "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001)\n",
            "\n",
            "Model # 18: \n",
            "RidgeCV(alphas=array([ 0.1,  1. , 10. ]), cv=None, fit_intercept=True,\n",
            "        gcv_mode=None, normalize=False, scoring=None, store_cv_values=False)\n",
            "\n",
            "Model # 19: \n",
            "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
            "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
            "             learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
            "             n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
            "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
            "             warm_start=False)\n",
            "\n",
            "Model # 20: \n",
            "TheilSenRegressor(copy_X=True, fit_intercept=True, max_iter=300,\n",
            "                  max_subpopulation=10000, n_jobs=None, n_subsamples=None,\n",
            "                  random_state=None, tol=0.001, verbose=False)\n",
            "\n",
            "Model # 21: \n",
            "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
            "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      presort=False, random_state=None, splitter='best')\n",
            "\n",
            "Model # 22: \n",
            "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
            "                      max_features='auto', max_leaf_nodes=None,\n",
            "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                      min_samples_leaf=1, min_samples_split=2,\n",
            "                      min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
            "                      n_jobs=None, oob_score=False, random_state=None,\n",
            "                      verbose=0, warm_start=False)\n",
            "\n",
            "Model # 23: \n",
            "AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
            "                  n_estimators=50, random_state=None)\n",
            "\n",
            "Model # 24: \n",
            "BaggingRegressor(base_estimator=None, bootstrap=True, bootstrap_features=False,\n",
            "                 max_features=1.0, max_samples=1.0, n_estimators=10,\n",
            "                 n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
            "                 warm_start=False)\n",
            "\n",
            "Model # 25: \n",
            "ExtraTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n",
            "                   max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
            "                   min_impurity_split=None, min_samples_leaf=1,\n",
            "                   min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                   random_state=None, splitter='random')\n",
            "\n",
            "Model # 26: \n",
            "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
            "          intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
            "          random_state=None, tol=0.0001, verbose=0)\n",
            "\n",
            "Model # 27: \n",
            "NuSVR(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma='auto_deprecated',\n",
            "      kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
            "      verbose=False)\n",
            "\n",
            "Model # 28: \n",
            "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
            "    gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeABl1fnGzcL",
        "colab_type": "text"
      },
      "source": [
        "### Fit and evalulate the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmT0IyG1Mh54",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e1de17f-8f24-4cdc-b255-f9dbc6eb135c"
      },
      "source": [
        "# set scalars dataframe as df for simplicity\n",
        "df = dic['scalars']\n",
        "\n",
        "# set conditions for keeping samples in training set and get the row indices\n",
        "train_samples = np.where(np.logical_or(\n",
        "    np.less(df['rh'], 6),\n",
        "    np.greater(df['rh'], 92)))\n",
        "train_samples = np.append(train_samples, np.arange(10)+100)\n",
        "\n",
        "# create dictionary for holding model scores\n",
        "scoredict = {}\n",
        "\n",
        "# loop over each target feature for prediction\n",
        "for tar in df.columns[1:]:\n",
        "    scoredict[tar] = {}\n",
        "\n",
        "    # find type of target feature and other similar features we should drop\n",
        "    tar_type, drop_features = find_key_of_value(tar, feature_dict)\n",
        "    print('\\n----------------------------------------------------')\n",
        "    print('target feature: %s, feature type: %s' %(tar, tar_type))\n",
        "\n",
        "    # split into testing and training and store in dataset dictionary\n",
        "    ds = format_for_ml(df0=df,\n",
        "                    target_feature=tar,\n",
        "                    train_samples=train_samples,\n",
        "                    drop_features=drop_features)\n",
        "    \n",
        "    # loop over each model in model dictionary and deplot the model\n",
        "    for model in modeldict:\n",
        "        model_results = deploy_model(\n",
        "            model=modeldict[model], dataset=ds, ind_var_name='rh')\n",
        "        \n",
        "        #plot_reg_results(ds, model_results, model_name=model, ind_var_name='rh')\n",
        "        #print('model score: %.2f' %(100*model_results['model_score']))\n",
        "        #print('avg. percent error: %.2f' %model_results['avg_percent_error'])\n",
        "\n",
        "        scoredict[tar][model] = model_results['avg_percent_error']\n",
        "\n",
        "# create dataframe from score dictionary\n",
        "scoredf = pd.DataFrame.from_dict(scoredict)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: df1, feature type: qcm\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.12027055933753195, tolerance: 0.09857443742473679\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.485e-05, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.419e-05, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.311e-05, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.189e-05, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.116e-05, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.526e-05, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.161e-05, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.082e-05, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.660e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.493e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.717e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.262e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.170e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.358e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.207e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.135e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.053e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.428e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.921e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.278e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.311e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.15553096646428544, tolerance: 0.08605589979326564\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.16946002998841614, tolerance: 0.08605589979326564\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.217527296136089, tolerance: 0.09857443742473679\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: dd1, feature type: qcm\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.661e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.301e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.194e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.873e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.722e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.549e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.122e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.052e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.695e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.985e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11244.649163344453, tolerance: 919.6745991834478\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 828.3939359278884, tolerance: 745.1130427769439\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 850.2569414019235, tolerance: 745.1130427769439\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2797.5515169369173, tolerance: 745.1130427769439\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3595.1118157642195, tolerance: 745.1130427769439\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1510.7276820004045, tolerance: 745.1130427769439\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1630.6450207491434, tolerance: 745.1130427769439\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1613.087272743658, tolerance: 745.1130427769439\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1563.7656220285135, tolerance: 745.1130427769439\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1505.6883967338945, tolerance: 745.1130427769439\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1444.463822687776, tolerance: 745.1130427769439\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1381.937185210525, tolerance: 745.1130427769439\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1319.0505270989015, tolerance: 745.1130427769439\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1256.4541909707987, tolerance: 745.1130427769439\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1194.649061476055, tolerance: 745.1130427769439\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1134.0295317885339, tolerance: 745.1130427769439\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: df7, feature type: qcm\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6270390430964274, tolerance: 0.22078084279615037\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.714e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.710e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.686e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.588e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.576e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 9.599e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.464e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.327e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.167e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.650e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=5.156e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.044e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.596e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.474e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.425e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.377e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.172e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.097e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 9.246e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.570e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 9.246e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.427e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.128e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.213e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.842e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.597e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.521e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.436e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.517e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.491e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.335e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.248e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.277e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.066e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.801e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.831e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.45497405371819133, tolerance: 0.22078084279615037\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: dd7, feature type: qcm\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.541e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.710e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.914e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.848e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.816e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.780e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.066e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.437e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.611e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.455e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 202.47768925443415, tolerance: 100.62109790906565\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 236.27081755574318, tolerance: 100.62109790906565\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.90054740402888, tolerance: 100.62109790906565\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 263.3321223508574, tolerance: 118.15498788324919\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: df11, feature type: qcm\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0024460179571442, tolerance: 0.8051971877781023\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.454e+00, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.282e+00, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.254e+00, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.026e-01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=5.114e-01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.974e-01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.936e-01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.830e-01, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.167e-01, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.012e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.938e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.885e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.837e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.655e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.354e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.304e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.853e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.659e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.573e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.543e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.091e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.755e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.154e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.453e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.661e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.883e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.608e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.795e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.787e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.786e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.454e+00, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.282e+00, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.254e+00, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.026e-01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=5.114e-01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.974e-01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.936e-01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.830e-01, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5759176293255397, tolerance: 0.8051971877781023\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4428968183279394, tolerance: 0.8051971877781023\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: dd11, feature type: qcm\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.729e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.313e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.634e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.577e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.550e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.519e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.101e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.200e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.789e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.241e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 84019.08148490469, tolerance: 2562.404132423383\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2157.696420323031, tolerance: 1944.8428110826367\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2429.171661073109, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2456.7870283604134, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6860.864178781805, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4418.682660312159, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5569.871255853039, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5726.182732359273, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5648.687002332415, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5503.788422034035, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5331.140675941075, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5142.883265981582, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4944.7407824109105, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4740.553145701153, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4533.302901226067, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4325.397794490855, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4118.788653399228, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3915.0448437823943, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3715.4138398518553, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3520.8713349491445, tolerance: 2152.009027091266\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: dmu1, feature type: qcm\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.289e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.094e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.810e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.634e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.704e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.552e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.235e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.134e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.682e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.091e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.364061075790847e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.643251512976024e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.287544550722088e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.95384103853278e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3705719526743006e-12, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.698813621367837e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.221750773650883e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.342389346900046e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.158131140616553e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.900683150032569e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.621941791549525e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.335790434486488e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.047670682359095e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.760798262154926e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.477520365580827e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.361792413547576e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.643169641778531e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.288779217558264e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.954523264253056e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.370665480647661e-12, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.699043494202392e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.221997757961672e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.342624327202167e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.158340284341955e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.900863826349694e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.622095281673981e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.335919091175024e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.047777063077628e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.760884862134843e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.477589542045634e-13, tolerance: 3.9614554332954833e-13\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: deta1, feature type: qcm\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.968e+03, with an active set of 14 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.468e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.468e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.468e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.467e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.467e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.467e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.467e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.467e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.466e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.464e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.755e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.702e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.680e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.612e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.608e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.509e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.467e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.460e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.447e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.567e+02, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.932e+02, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.757e+02, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.486e+02, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.113e+02, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.855e+02, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.794e+02, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.435e+02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.406e+02, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.858e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.560e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.715e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.510e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.032e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.929e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.895e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.827e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.733e+00, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2759455081.11429, tolerance: 2523262108.4571457\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.468e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.468e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.468e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.467e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.467e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.467e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.467e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.467e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.466e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.464e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.879e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.879e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.870e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.865e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.855e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.846e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.799e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.731e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.640e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.624e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.826e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.823e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.821e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.811e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.804e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.743e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.719e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.672e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.535e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.462e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.826e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.823e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.821e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.811e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.804e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.743e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.719e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.672e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.535e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.462e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: dmu7, feature type: qcm\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.573788561878019e-11, tolerance: 4.164999536359939e-11\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.592980078496132e-11, tolerance: 4.164999536359939e-11\n",
            "  tol, rng, random, positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: deta7, feature type: qcm\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.848e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.816e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.708e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.163e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.028e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.018e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.984e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.796e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.152e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.629e-06, with an active set of 3 regressors, and the smallest cholesky pivot element being 7.814e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.629e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.628e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.274e+03, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.244e+03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.220e+03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.215e+03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.164e+03, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.130e+03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.100e+03, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.099e+03, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.092e+03, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.907e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 9.246e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.882e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.575e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.566e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.554e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.551e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.508e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.580e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.507e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.580e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.493e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.372e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 9.771e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=8.818e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=8.433e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.068e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.773e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.002e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.848e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.816e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.708e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.163e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.028e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.018e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.984e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.796e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.152e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 41564201579.15712, tolerance: 20331480448.5319\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.853e-07, with an active set of 3 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.852e-07, with an active set of 3 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.852e-07, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.851e-07, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.851e-07, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.850e-07, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.849e-07, with an active set of 3 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.846e-07, with an active set of 3 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.844e-07, with an active set of 3 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.839e-07, with an active set of 3 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.838e-07, with an active set of 3 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.922e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.900e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.895e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.892e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.886e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=3.897e-07, previous alpha=3.880e-07, with an active set of 5 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.922e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.900e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.895e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.892e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.886e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=3.897e-07, previous alpha=3.880e-07, with an active set of 5 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: dmu11, feature type: qcm\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: deta11, feature type: qcm\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=6.175e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.678e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.652e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.583e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.394e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.625e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.430e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.340e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.324e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.266e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.208e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.185e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.167e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=6.175e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.678e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.652e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.583e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.394e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.625e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: cvarea10, feature type: dc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.030226369787838235, tolerance: 0.005893368378608599\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.005000209278070388, tolerance: 0.0021364091252110027\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.584e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.500e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.488e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.466e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.460e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.413e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.372e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.219e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.749e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.659e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.354e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.266e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.250e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.194e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.137e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.114e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.096e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.072e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.061e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.626e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.574e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.566e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.594e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=8.865e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.350e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.313e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.289e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.274e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.234e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.117e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=9.099e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=9.065e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.143e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.232e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.619e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.322e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.263e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.987e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=8.821e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.584e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.500e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.488e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.466e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.460e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.413e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.372e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.219e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.749e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.659e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: cvarea50, feature type: dc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0071438332082619255, tolerance: 0.0021364091252110027\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.002085766525535473, tolerance: 0.0006453860324161603\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.422e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.444e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.332e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.322e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.301e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.264e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.216e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.188e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.010e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.079e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.192e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.336e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.039e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.803e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.803e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=9.875e-07, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=9.875e-07, with an active set of 8 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.078e-07, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.171e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.157e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.151e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.114e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.999e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.939e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.880e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.873e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.745e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.694e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.590e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.466e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.697e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.591e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.569e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.463e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.149e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.946e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.245e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.621e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.469e-06, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.996e-06, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.507e-06, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.084e-06, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.959e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 9.771e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.816e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.604e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.959e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 7.376e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: cvarea100, feature type: dc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0009026365299487486, tolerance: 0.0006453860324161603\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.137458182075112e-05, tolerance: 1.8295362095183658e-05\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.591e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.499e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.381e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.298e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.083e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.873e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.154e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.967e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.560e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.235e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.670e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.442e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.848e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.727e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.660e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.656e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.518e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.405e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.024e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.950e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.558e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.271e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.974e-06, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: cvarea300, feature type: dc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.8919665421078944e-05, tolerance: 1.8295362095183658e-05\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.006861758657823923, tolerance: 0.006155916091482847\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.281e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.222e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.716e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.284e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.488e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.235e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.948e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.343e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.281e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.270e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.231e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.191e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.175e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.163e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: cvmax10, feature type: dc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.078e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.056e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.997e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.815e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.694e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.409e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.605e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.389e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.934e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.121e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.125e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.210e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.189e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.479e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.429e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.211e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=8.059e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=6.976e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=6.204e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=6.052e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.359e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.146e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.488e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.814e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.140e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.189e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=9.513e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.567e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02465046020987953, tolerance: 0.006155916091482847\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: cvmax50, feature type: dc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.806e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.800e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.796e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.783e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.774e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.769e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.712e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.693e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.687e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.507e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.264e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.912e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.681e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.610e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.598e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.553e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.507e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.489e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.475e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.171e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.151e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.116e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.024e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.012e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.945e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.713e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02295183760974505, tolerance: 0.005460512592911747\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0032037773837057673, tolerance: 0.0014262623312959995\n",
            "  positive)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: cvmax100, feature type: dc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.805e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.781e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.659e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.506e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.501e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.455e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.354e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.268e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.251e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.083e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=5.059e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.188e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.736e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.724e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.719e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.690e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.598e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.550e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.503e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.497e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.395e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.354e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.271e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.172e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.955e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.870e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.853e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.768e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.517e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.355e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.795e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.295e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.569e-06, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.793e-06, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.803e-06, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.662e-07, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.009e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 9.771e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.886e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.578e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=8.642e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 7.376e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0014486821374057827, tolerance: 0.0014262623312959995\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.00016314635117756871, tolerance: 7.499700470984472e-05\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.332e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.318e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.286e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.168e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.165e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.158e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.141e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.138e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.103e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.848e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.100e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.432e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.145e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.545e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.591e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.169e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.074e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.936e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.756e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.749e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.643e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: cvmax300, feature type: dc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0001522842189314672, tolerance: 7.499700470984472e-05\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.00011024693502307935, tolerance: 7.801938660177788e-05\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.430e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.378e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.368e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.366e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.362e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.359e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.352e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.342e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.325e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.128e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.898e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.856e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.392e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.237e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.082e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.732e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.732e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.185e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.412e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.093e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.546e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: ivmaxcurr, feature type: dc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0020772805693991046, tolerance: 0.0006903664746414462\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.216e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.208e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.190e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.162e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.339e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.281e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.155e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=8.580e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.797e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.815e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.021e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: ivmincurr, feature type: dc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.008e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.002e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.968e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.861e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.805e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.750e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.743e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.624e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.577e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.480e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.364e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.441e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.343e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.322e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.223e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.930e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.742e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.090e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.508e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.813e-06, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.581e-06, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.264e-06, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.009e-06, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.570e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 9.771e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.434e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.234e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.625e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 7.376e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.00170216186284214, tolerance: 0.0006903664746414462\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17566652614.550293, tolerance: 1834690669.0338166\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.762e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.025e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.645e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.533e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.389e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.373e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.081e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.030e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.603e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.587e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.533e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.942e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.691e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.483e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.020e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.311e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.843e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.770e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.543e+02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.193e+01, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.163e+01, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: z1hz, feature type: ac\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: phi1hz, feature type: ac\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5505769656782071, tolerance: 0.687291465461538\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.100e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.923e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.832e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.805e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.771e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.767e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.697e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.445e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.343e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.339e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.326e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.184e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.124e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.074e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.237e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.539e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.415e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.241e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.697e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.255e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.788e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7238597950103967e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.487141366808114e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.6779562623429536e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.68524391992117e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.6560709027496006e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.61337032531458e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5612740509387503e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.361121889330661e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4055876155708233e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3876713622634124e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3799405557361953e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4152806820159046e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.541380936534342e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3293695467993456e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0418288123347123e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.020586388734427e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.738768288091267e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.2337154222725083e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.5538293564258243e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.738964689401616e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.820593886110971e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.823458341720756e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5863330373271553e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5303329520277897e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.9982928672375896e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.9125151915933883e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5190010466586237e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.480535254527181e-08, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.358616811064487e-08, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7649349347623997e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7853425584842994e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2288511557886085e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.437183973224531e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5217222765022817e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.475898542210431e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.1798911710765067e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.9731588034256437e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.780992196808872e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.602052726569479e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4351539174908874e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.279299546140993e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1336325930583158e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9974040251633836e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8699498069919867e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7506739312347793e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6390358223968057e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5345409226774628e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4367335987409673e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3451917438604462e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.259522619509212e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1793596077964647e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1043596321064412e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: n600, feature type: optical\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.581e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.955e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.894e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.883e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.704e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.690e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.480e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.409e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.220e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.200e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.190e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.875e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.770e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.356e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.230e-06, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.854e-07, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.587e-07, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.372e-07, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.156e-07, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.102e-07, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.963e-07, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.904e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.742e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.738e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.275e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.105e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.973e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.965e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.733e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.704e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.597e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.446e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.598560600291101e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5847042914367736e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.918094565649682e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.034531149734996e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.0817883209665667e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.0862200873117323e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4201983463141756e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4621228625357823e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4342569246311844e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4049485004919517e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3984782586897372e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4371416393772542e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5486541002147534e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0771818078073697e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.1033573802428873e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.8592484880755053e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.380279993626304e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.7157455948321345e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.907095470961571e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.987576819439783e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.98365073263541e-07, tolerance: 1.000213893117645e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5742109574929248e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6395850259100534e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.1445414879435705e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0312194268753906e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.575234171603571e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4698876840748082e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.920860405736535e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0204566976452113e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.513556398338483e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.710166446082672e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.74144047439404e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.648828474650709e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.373738546689296e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.1986932070800073e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.028552231979033e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.8642236916702607e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.706048029283293e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.554257013140943e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.408982231730985e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2702728655880012e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1381108478952316e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0124237457442985e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8930956843855132e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7799765876674694e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.672889972868518e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5716395032901284e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.476014477275108e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3857944045441295e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3007528040266203e-07, tolerance: 4.6620505356944464e-08\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.563885511788243e-07, tolerance: 2.2703985901689896e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.871e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.865e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.817e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.817e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.803e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.799e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.799e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.794e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.784e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.762e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.733e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.732e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.721e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.501e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.455e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=9.493e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=9.082e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=8.465e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.808e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.009e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.720e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.425e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.716e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.429e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.332e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.882e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.646e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.491e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.008e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.039e-07, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.917e-07, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------\n",
            "target feature: k600, feature type: optical\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.659735774412442e-07, tolerance: 2.2703985901689896e-07\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/omp.py:387: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
            "dependence in the dictionary. The requested precision might not have been met.\n",
            "\n",
            "  copy_X=copy_X, return_path=return_path)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8Aw3sinGxG_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "5c6058f5-d6a0-4cc4-acfc-822768dc6603"
      },
      "source": [
        "def df_to_heatmap(df):\n",
        "    '''\n",
        "    Plot a heatmap from 2D data in a Pandas DataFrame. The y-axis labels \n",
        "    should be index names, and x-axis labels should be column names.\n",
        "    '''\n",
        "    plt.pcolor(scoredf, cmap='jet', vmin=0, vmax=50)\n",
        "    plt.yticks(np.arange(0.5, len(scoredf.index), 1), df.index)\n",
        "    plt.xticks(np.arange(0.5, len(scoredf.columns), 1),\n",
        "               df.columns, rotation='vertical')\n",
        "    fig = plt.gcf()\n",
        "    fig.set_size_inches(7, 7)\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "df_to_heatmap(scoredf)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn0AAAHtCAYAAABs/cgTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 504x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNB-sG4fXxg6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "97f2d423-7dc7-4e13-c24b-9202f1ad2c1c"
      },
      "source": [
        "from matplotlib import cm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for tar in scoredict:\n",
        "\n",
        "    colors = cm.Set3(np.arange(len(scoredict[tar].values())) / len(scoredict[tar]))\n",
        "\n",
        "    scoredf[tar].loc[]\n",
        "\n",
        "\n",
        "'''\n",
        "    plt.bar(np.arange(len(scoredict[tar])),\n",
        "            list(scoredict[tar].values()),\n",
        "            align='center', width=0.5, color=colors)\n",
        "\n",
        "    plt.xticks(np.arange(len(scoredict[tar])),\n",
        "               list(scoredict[tar].keys()),\n",
        "               rotation='vertical')\n",
        "\n",
        "    fig = plt.gcf()\n",
        "    fig.set_size_inches(10, 4)\n",
        "    plt.title(tar.upper(), fontsize=16)\n",
        "    plot_setup(labels=['Model', 'Avg. residual (%)'],\n",
        "               setlimits=True,\n",
        "               limits=[-0.5, len(scoredict[tar])-0.5, 0, 100])\n",
        "    plt.show()\n",
        "\n",
        "'''"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n    plt.bar(np.arange(len(scoredict[tar])),\\n            list(scoredict[tar].values()),\\n            align='center', width=0.5, color=colors)\\n\\n    plt.xticks(np.arange(len(scoredict[tar])),\\n               list(scoredict[tar].keys()),\\n               rotation='vertical')\\n\\n    fig = plt.gcf()\\n    fig.set_size_inches(10, 4)\\n    plt.title(tar.upper(), fontsize=16)\\n    plot_setup(labels=['Model', 'Avg. residual (%)'],\\n               setlimits=True,\\n               limits=[-0.5, len(scoredict[tar])-0.5, 0, 100])\\n    plt.show()\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-yY-T2m7UtO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "3d3b3aec-6095-4281-f6d1-e8048af74602"
      },
      "source": [
        "scoredf.loc['lars']"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "df1          NaN\n",
              "dd1          NaN\n",
              "df7          NaN\n",
              "dd7          NaN\n",
              "df11         NaN\n",
              "dd11         NaN\n",
              "dmu1         NaN\n",
              "deta1        NaN\n",
              "dmu7         NaN\n",
              "deta7        NaN\n",
              "dmu11        NaN\n",
              "deta11       NaN\n",
              "cvarea10     NaN\n",
              "cvarea50     NaN\n",
              "cvarea100    NaN\n",
              "cvarea300    NaN\n",
              "cvmax10      NaN\n",
              "cvmax50      NaN\n",
              "cvmax100     NaN\n",
              "cvmax300     NaN\n",
              "ivmaxcurr    NaN\n",
              "ivmincurr    NaN\n",
              "z1hz         NaN\n",
              "phi1hz       NaN\n",
              "n600         NaN\n",
              "k600         NaN\n",
              "Name: lars, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkZyWsRXIHJ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "74bd548b-7955-4b08-8b55-79c6d3049863"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEBCAYAAAB7Wx7VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBGUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awxdxYuzuJHY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae18f2a0-5d6c-4b0c-e810-f4dd8182fd1a"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0, 100],\n",
              "       [  1, 101],\n",
              "       [  2, 102],\n",
              "       [  3, 103],\n",
              "       [  4, 104],\n",
              "       [  5, 105],\n",
              "       [  6, 106],\n",
              "       [  7, 107],\n",
              "       [  8, 108],\n",
              "       [  9, 109],\n",
              "       [ 10, 110],\n",
              "       [ 11, 111],\n",
              "       [ 12, 112],\n",
              "       [ 13, 113],\n",
              "       [ 14, 114],\n",
              "       [ 15, 115],\n",
              "       [ 16, 116],\n",
              "       [ 17, 117],\n",
              "       [ 18, 118],\n",
              "       [ 19, 119],\n",
              "       [ 20, 120],\n",
              "       [ 21, 121],\n",
              "       [ 22, 122],\n",
              "       [ 23, 123],\n",
              "       [ 24, 124],\n",
              "       [ 25, 125],\n",
              "       [ 26, 126],\n",
              "       [ 27, 127],\n",
              "       [ 28, 128],\n",
              "       [ 29, 129],\n",
              "       [ 30, 130],\n",
              "       [ 31, 131],\n",
              "       [ 32, 132],\n",
              "       [ 33, 133],\n",
              "       [ 34, 134],\n",
              "       [ 35, 135],\n",
              "       [ 36, 136],\n",
              "       [ 37, 137],\n",
              "       [ 38, 138],\n",
              "       [ 39, 139],\n",
              "       [ 40, 140],\n",
              "       [ 41, 141],\n",
              "       [ 42, 142],\n",
              "       [ 43, 143],\n",
              "       [ 44, 144],\n",
              "       [ 45, 145],\n",
              "       [ 46, 146],\n",
              "       [ 47, 147],\n",
              "       [ 48, 148],\n",
              "       [ 49, 149],\n",
              "       [ 50, 150],\n",
              "       [ 51, 151],\n",
              "       [ 52, 152],\n",
              "       [ 53, 153],\n",
              "       [ 54, 154],\n",
              "       [ 55, 155],\n",
              "       [ 56, 156],\n",
              "       [ 57, 157],\n",
              "       [ 58, 158],\n",
              "       [ 59, 159],\n",
              "       [ 60, 160],\n",
              "       [ 61, 161],\n",
              "       [ 62, 162],\n",
              "       [ 63, 163],\n",
              "       [ 64, 164],\n",
              "       [ 65, 165],\n",
              "       [ 66, 166],\n",
              "       [ 67, 167],\n",
              "       [ 68, 168],\n",
              "       [ 69, 169],\n",
              "       [ 70, 170],\n",
              "       [ 71, 171],\n",
              "       [ 72, 172],\n",
              "       [ 73, 173],\n",
              "       [ 74, 174],\n",
              "       [ 75, 175],\n",
              "       [ 76, 176],\n",
              "       [ 77, 177],\n",
              "       [ 78, 178],\n",
              "       [ 79, 179],\n",
              "       [ 80, 180],\n",
              "       [ 81, 181],\n",
              "       [ 82, 182],\n",
              "       [ 83, 183],\n",
              "       [ 84, 184],\n",
              "       [ 85, 185],\n",
              "       [ 86, 186],\n",
              "       [ 87, 187],\n",
              "       [ 88, 188],\n",
              "       [ 89, 189],\n",
              "       [ 90, 190],\n",
              "       [ 91, 191],\n",
              "       [ 92, 192],\n",
              "       [ 93, 193],\n",
              "       [ 94, 194],\n",
              "       [ 95, 195],\n",
              "       [ 96, 196],\n",
              "       [ 97, 197],\n",
              "       [ 98, 198],\n",
              "       [ 99, 199]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXox1rm7LKPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSojv1BWLKMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxGPjeFwLJ_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSUN24DAuIXP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA7Xy19M1Uku",
        "colab_type": "text"
      },
      "source": [
        "# Build models in Keras / Tensorflow\n",
        "\n",
        "For building a neural network regressino model, we will use Keras, a wrapper for Google's \n",
        "Tensorflow library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCIBSctbkLVU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7a85381-b87e-4ddf-b7ab-15fe9d69e4cc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from numpy.random import seed\n",
        "from tensorflow import set_random_seed\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras.callbacks import Callback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from timeit import default_timer as timer"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6rWwNQZ9s9R",
        "colab_type": "text"
      },
      "source": [
        " ## Fix the random number generators\n",
        "\n",
        " Ransom number generators are often used to create initial values for\n",
        " weights and biases of the model. Here we fix the numpy and tensorflow random\n",
        "  number generators so we can reproduce the model results each time we run the\n",
        "  code. This is not esential but it helps for judging the model performance \n",
        "  when a model parameter is changed, since our model will always begin in\n",
        "  the same initialization state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0x0KZ0J94xD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed(1)\n",
        "set_random_seed(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHNSmbYXHICT",
        "colab_type": "text"
      },
      "source": [
        "## Create a callback function\n",
        "\n",
        "When we train an artificial neural network, it is difficult to tell how many\n",
        "layers and how many neurons to use. To get some insights, we want to know how\n",
        "the weights and biases of each layer and neuron are changing during training.\n",
        "\n",
        "For reading the value of weights at each training epoch, we write a special\n",
        "Keras callback function which runs at each epoch and stores the weights and\n",
        "biases of each neuron in every layer. This will be called when we fit the\n",
        "neural network model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNjLzcvkHGQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GetWeights(Callback):\n",
        "    '''\n",
        "    Custom Keras callback which collects values of weights and biases\n",
        "    after each epoch of training the model. Should be used like this:\n",
        "    gw = GetWeights()\n",
        "    model.fit(callbacks=[gw])\n",
        "    wd = gw.weight_dict\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(GetWeights, self).__init__()\n",
        "        self.weight_dict = {}\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # this function runs at the end of each epoch\n",
        "        # loop over each layer and get weights and biases\n",
        "        for layer_i in range(len(self.model.layers)):\n",
        "            w = self.model.layers[layer_i].get_weights()[0]\n",
        "            b = self.model.layers[layer_i].get_weights()[1]\n",
        "            # save all weights and biases inside a dictionary\n",
        "            if epoch == 0:\n",
        "                # create array to hold weights and biases\n",
        "                self.weight_dict['w_'+str(layer_i+1)] = w\n",
        "                self.weight_dict['b_'+str(layer_i+1)] = b\n",
        "            else:\n",
        "                # append new weights to previously-created weights array\n",
        "                self.weight_dict['w_'+str(layer_i+1)] = np.dstack(\n",
        "                    (self.weight_dict['w_'+str(layer_i+1)], w))\n",
        "                # append new weights to previously-created weights array\n",
        "                self.weight_dict['b_'+str(layer_i+1)] = np.dstack(\n",
        "                    (self.weight_dict['b_'+str(layer_i+1)], b)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQbtBS53H1OB",
        "colab_type": "text"
      },
      "source": [
        "Now we can build the structure of the neural network, set the optimizers,\n",
        "compile the model, and finally fit (train) it. We plot the history of the model\n",
        "during training to track how many epochs it should complete before the\n",
        "model starts overtraining (validation loss begins increasing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1vHfVMNIWkY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "1311b778-02ff-4bbb-d146-071ef93405e2"
      },
      "source": [
        "# instantiate our previously-defined callback class       \n",
        "gw = GetWeights()\n",
        "\n",
        "# create the neural network structure\n",
        "train_start_time = timer()\n",
        "model = Sequential([\n",
        "                    Dense(24, activation='relu',\n",
        "                          input_shape=(train_inp_s.shape[1:])),\n",
        "\n",
        "                    #Dense(4, activation='relu'),\n",
        "                    #Dense(8, activation='relu',\n",
        "                    #Dense(4, activation='relu'),\n",
        "                    #Dropout(.2, seed=1),\n",
        "                  Dense(1)])\n",
        "\n",
        "# set hyperparameters related to the stochastic gradient descent optimizer\n",
        "sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "# compile the model with optimizer and loss function\n",
        "model.compile(loss='mean_squared_error', optimizer=sgd)\n",
        "\n",
        "# train the model\n",
        "epochs = 5\n",
        "\n",
        "history = model.fit(train_inp_s, train_tar_s,\n",
        "                   validation_split=0.15,\n",
        "                   epochs=epochs,\n",
        "                   batch_size=500,\n",
        "                   verbose=1,\n",
        "                   callbacks=[gw])\n",
        "print('training time = %.1f sec (%.2f min)' %(timer()-train_start_time,\n",
        "                                 (timer()-train_start_time)/60))\n",
        "\n",
        "# plot loss during training\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "plt.semilogy(loss, c='k')\n",
        "plt.semilogy(val_loss, c='r')\n",
        "plot_setup(['Epoch', 'Loss'])\n",
        "plt.legend(['training', 'validation'])\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-4991d24fcf5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m model = Sequential([\n\u001b[1;32m      6\u001b[0m                     Dense(24, activation='relu',\n\u001b[0;32m----> 7\u001b[0;31m                           input_shape=(train_inp_s.shape[1:])),\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0;31m#Dense(4, activation='relu'),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_inp_s' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnw3_W-cNsDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib import cm\n",
        "\n",
        "wd = gw.weight_dict\n",
        "\n",
        "# loop over each epoch in the training session\n",
        "for epoch in range(0, 3, 1):\n",
        "    # layout the figure based on the number of layers\n",
        "    fig = plt.figure(figsize=(15, 6))\n",
        "    gs_stretch = 6\n",
        "    gs = gridspec.GridSpec(6, int(len(wd)/2)*gs_stretch, figure=fig)\n",
        "    gs.update(left=0.1, right=0.9, top=0.85, bottom=0.03, wspace=0.01, hspace=2)\n",
        "\n",
        "    # loop over each array of weights and biases\n",
        "    for key_i, key in enumerate(wd):\n",
        "        # print(str(key) + ' shape: %s' %str(np.shape(gw.weight_dict[key])))\n",
        "        # check if key is a weights key\n",
        "        if 'w' in key:\n",
        "            # row and column to place plot\n",
        "            row, col = gs.get_geometry()[0]-1, gs_stretch*int(key_i/2)+int(key_i/2)\n",
        "            # normal layer\n",
        "            if key != list(wd.keys())[-2]:\n",
        "                plt.subplot(gs[:row, col:col+gs_stretch]).axis('off')\n",
        "                plt.gca().set_title('L-'+str(1+int(key_i/2))+' weights', fontsize=18)\n",
        "            # output layer\n",
        "            else:\n",
        "                plt.subplot(gs[:row, col]).axis('off')\n",
        "                plt.gca().set_title('Output\\nweights', fontsize=18)\n",
        "        # check if key is a biases key\n",
        "        if 'b' in key:\n",
        "            # normal layer\n",
        "            if key != list(wd.keys())[-1]:\n",
        "                plt.subplot(gs[row, col:col+gs_stretch]).axis('off') \n",
        "                plt.gca().set_title('L-'+str(1+int(key_i/2))+' biases', fontsize=18) \n",
        "            # output layer\n",
        "            else:\n",
        "                plt.subplot(gs[row, col]).axis('off')\n",
        "                plt.gca().set_title('Output\\nbiases', fontsize=18)\n",
        "\n",
        "        # generate the heatmap of weight/bias values\n",
        "        plt.imshow(wd[key][:,:,epoch],\n",
        "                aspect='auto',\n",
        "                cmap=plt.get_cmap('coolwarm'),\n",
        "                interpolation='nearest')\n",
        "\n",
        "    fig.suptitle('Epoch '+str(epoch), fontsize=22)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    # save the image to file\n",
        "    save_heatmaps = False\n",
        "    if save_heatmaps:\n",
        "        fig_filename = 'epoch_'+str(epoch).zfill(5)+'.png'\n",
        "        fig.savefig(fig_filename, dpi=200)\n",
        "        files.download(fig_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTgs61NLBLy0",
        "colab_type": "text"
      },
      "source": [
        "## Animate the weights into a video clip\n",
        "\n",
        "We want to visualize how the neural network model weights and biases are \n",
        "changing as the model is trained so we can tell which layers are \n",
        "superfluous and merely contribute to longer training times and overfitting.\n",
        "\n",
        "We use the OpenCV library (imported as cv2) to compile the saved images of our\n",
        " weights in a short video. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwL4DQx9NwU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glob import glob\n",
        "import cv2\n",
        "\n",
        "def create_video(image_list, video_name, fps=8, reverse=False):\n",
        "    # create video out of images saved in a folder\n",
        "    # frames per second (fps) and order of the images can be reversed \n",
        "    # using the **kwargs.\n",
        "    if reverse: image_list = image_list[::-1]\n",
        "    frame = cv2.imread(image_list[0])\n",
        "    height, width, layers = frame.shape\n",
        "    video = cv2.VideoWriter(video_name, -1, fps, (width,height))\n",
        "    for image in image_list:\n",
        "        video.write(cv2.imread(image))\n",
        "    cv2.destroyAllWindows()\n",
        "    video.release()\n",
        "    return video\n",
        "\n",
        "image_list = sorted(glob(r'C:\\Users\\a6q\\exp_data\\ANN_weight_maps\\/*.png'))\n",
        "\n",
        "make_video = False\n",
        "if make_video:\n",
        "    video = create_video(\n",
        "        image_list, r'C:\\Users\\a6q\\Desktop\\ANN_weightmap.avi', fps=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cADVI8HjZ23v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WSgFrNrIiHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#%% make predictions\n",
        "trained_model_s = model.predict(train_inp_s), \n",
        "future_predictions_s = model.predict(test_inp_s)\n",
        "\n",
        "#%% get sparse matrices which have same shape as scaled \n",
        "# training/testing data so we can unscale ANN results\n",
        "# so we can unscale using the same scaler as before\n",
        "trained_model_mat_s = np.zeros((train_size,len(input_mat[0])))\n",
        "future_predictions_mat_s = np.zeros((test_size,len(input_mat[0])))\n",
        "test_tar_mat_s = np.copy(future_predictions_mat_s)\n",
        "\n",
        "# insert our data into its corresponding column of each sparse matrix\n",
        "trained_model_mat_s[:,tar_col] = np.ndarray.flatten(np.array(trained_model_s))\n",
        "future_predictions_mat_s[:,tar_col] = np.ndarray.flatten(future_predictions_s)\n",
        "\n",
        "\n",
        "#%%# unscale our data\n",
        "trained_model = scaler.inverse_transform(trained_model_mat_s)[:,tar_col]\n",
        "future_pred = scaler.inverse_transform(future_predictions_mat_s)[:,tar_col]\n",
        "train_tar = sig[:train_size]\n",
        "\n",
        "plt.plot(time[train_size:],future_pred, c='r', label='prediction')\n",
        "plt.scatter(time, sig, c='k', s=3, alpha=0.1, label='measured')\n",
        "plt.plot(time[:train_size], trained_model, c='b', alpha=0.5, label='model')\n",
        "plt.xlabel('Time (min)', fontsize=label_size)\n",
        "plt.ylabel('Amplitude', fontsize=label_size)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#%% make error calculations\n",
        "tot_model = np.append(trained_model, future_pred)\n",
        "error_raw = np.subtract(sig, tot_model)\n",
        "percent_error = 100*np.abs(error_raw)/np.max(np.abs(sig))\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "ax1.plot(time, pressure, linewidth=0.5, c='b')\n",
        "ax1.set_xlabel('Time (min)', fontsize=label_size)\n",
        "ax1.set_ylabel('Pressure', color='b', fontsize=label_size)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(time, percent_error, linewidth=0.5, c='r')\n",
        "ax2.set_ylabel('% error', color='r', fontsize=label_size)\n",
        "ax2.tick_params('y', colors='r')\n",
        "plt.show()\n",
        "\n",
        "print('avg. percent error = %.2f' %(np.mean(percent_error)))\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6XuzcunqUlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "from sklearn import cluster, covariance, manifold\n",
        "\n",
        "# #############################################################################\n",
        "# Retrieve the data from Internet\n",
        "\n",
        "# The data is from 2003 - 2008. This is reasonably calm: (not too long ago so\n",
        "# that we get high-tech firms, and before the 2008 crash). This kind of\n",
        "# historical data can be obtained for from APIs like the quandl.com and\n",
        "# alphavantage.co ones.\n",
        "\n",
        "symbol_dict = {\n",
        "    'TOT': 'Total',\n",
        "    'XOM': 'Exxon',\n",
        "    'CVX': 'Chevron',\n",
        "    'COP': 'ConocoPhillips',\n",
        "    'VLO': 'Valero Energy',\n",
        "    'MSFT': 'Microsoft',\n",
        "    'IBM': 'IBM',\n",
        "    'TWX': 'Time Warner',\n",
        "    'CMCSA': 'Comcast',\n",
        "    'CVC': 'Cablevision',\n",
        "    'YHOO': 'Yahoo',\n",
        "    'DELL': 'Dell',\n",
        "    'HPQ': 'HP',\n",
        "    'AMZN': 'Amazon',\n",
        "    'TM': 'Toyota',\n",
        "    'CAJ': 'Canon',\n",
        "    'SNE': 'Sony',\n",
        "    'F': 'Ford',\n",
        "    'HMC': 'Honda',\n",
        "    'NAV': 'Navistar',\n",
        "    'NOC': 'Northrop Grumman',\n",
        "    'BA': 'Boeing',\n",
        "    'KO': 'Coca Cola',\n",
        "    'MMM': '3M',\n",
        "    'MCD': 'McDonald\\'s',\n",
        "    'PEP': 'Pepsi',\n",
        "    'K': 'Kellogg',\n",
        "    'UN': 'Unilever',\n",
        "    'MAR': 'Marriott',\n",
        "    'PG': 'Procter Gamble',\n",
        "    'CL': 'Colgate-Palmolive',\n",
        "    'GE': 'General Electrics',\n",
        "    'WFC': 'Wells Fargo',\n",
        "    'JPM': 'JPMorgan Chase',\n",
        "    'AIG': 'AIG',\n",
        "    'AXP': 'American express',\n",
        "    'BAC': 'Bank of America',\n",
        "    'GS': 'Goldman Sachs',\n",
        "    'AAPL': 'Apple',\n",
        "    'SAP': 'SAP',\n",
        "    'CSCO': 'Cisco',\n",
        "    'TXN': 'Texas Instruments',\n",
        "    'XRX': 'Xerox',\n",
        "    'WMT': 'Wal-Mart',\n",
        "    'HD': 'Home Depot',\n",
        "    'GSK': 'GlaxoSmithKline',\n",
        "    'PFE': 'Pfizer',\n",
        "    'SNY': 'Sanofi-Aventis',\n",
        "    'NVS': 'Novartis',\n",
        "    'KMB': 'Kimberly-Clark',\n",
        "    'R': 'Ryder',\n",
        "    'GD': 'General Dynamics',\n",
        "    'RTN': 'Raytheon',\n",
        "    'CVS': 'CVS',\n",
        "    'CAT': 'Caterpillar',\n",
        "    'DD': 'DuPont de Nemours'}\n",
        "\n",
        "\n",
        "symbols, names = np.array(sorted(symbol_dict.items())).T\n",
        "\n",
        "quotes = []\n",
        "\n",
        "for symbol in symbols:\n",
        "    print('Fetching quote history for %r' % symbol, file=sys.stderr)\n",
        "    url = ('https://raw.githubusercontent.com/scikit-learn/examples-data/'\n",
        "           'master/financial-data/{}.csv')\n",
        "    quotes.append(pd.read_csv(url.format(symbol)))\n",
        "    print(len(pd.read_csv(url.format(symbol))))\n",
        "close_prices = np.vstack([q['close'] for q in quotes])\n",
        "open_prices = np.vstack([q['open'] for q in quotes])\n",
        "\n",
        "print(np.shape(close_prices))\n",
        "print(np.shape(open_prices))\n",
        "\n",
        "# The daily variations of the quotes are what carry most information\n",
        "variation = close_prices - open_prices\n",
        "print(np.shape(variation))\n",
        "#variation = np.random.random((len(names), 9))\n",
        "\n",
        "# #############################################################################\n",
        "# Learn a graphical structure from the correlations\n",
        "edge_model = covariance.GraphicalLassoCV(cv=5)\n",
        "\n",
        "# standardize the time series: using correlations rather than covariance\n",
        "# is more efficient for structure recovery\n",
        "X = variation.copy().T\n",
        "X /= X.std(axis=0)\n",
        "edge_model.fit(X)\n",
        "\n",
        "# #############################################################################\n",
        "# Cluster using affinity propagation\n",
        "\n",
        "_, labels = cluster.affinity_propagation(edge_model.covariance_)\n",
        "n_labels = labels.max()\n",
        "\n",
        "for i in range(n_labels + 1):\n",
        "    print('Cluster %i: %s' % ((i + 1), ', '.join(names[labels == i])))\n",
        "\n",
        "# #############################################################################\n",
        "# Find a low-dimension embedding for visualization: find the best position of\n",
        "# the nodes (the stocks) on a 2D plane\n",
        "\n",
        "# We use a dense eigen_solver to achieve reproducibility (arpack is\n",
        "# initiated with random vectors that we don't control). In addition, we\n",
        "# use a large number of neighbors to capture the large-scale structure.\n",
        "node_position_model = manifold.LocallyLinearEmbedding(\n",
        "    n_components=2, eigen_solver='dense', n_neighbors=6)\n",
        "\n",
        "embedding = node_position_model.fit_transform(X.T).T\n",
        "\n",
        "# #############################################################################\n",
        "# Visualization\n",
        "plt.figure(1, facecolor='w', figsize=(10, 8))\n",
        "plt.clf()\n",
        "ax = plt.axes([0., 0., 1., 1.])\n",
        "plt.axis('off')\n",
        "\n",
        "# Display a graph of the partial correlations\n",
        "partial_correlations = edge_model.precision_.copy()\n",
        "d = 1 / np.sqrt(np.diag(partial_correlations))\n",
        "partial_correlations *= d\n",
        "partial_correlations *= d[:, np.newaxis]\n",
        "non_zero = (np.abs(np.triu(partial_correlations, k=1)) > 0.02)\n",
        "\n",
        "# Plot the nodes using the coordinates of our embedding\n",
        "plt.scatter(embedding[0], embedding[1], s=100 * d ** 2, c=labels,\n",
        "            cmap=plt.cm.nipy_spectral)\n",
        "\n",
        "# Plot the edges\n",
        "start_idx, end_idx = np.where(non_zero)\n",
        "# a sequence of (*line0*, *line1*, *line2*), where::\n",
        "#            linen = (x0, y0), (x1, y1), ... (xm, ym)\n",
        "segments = [[embedding[:, start], embedding[:, stop]]\n",
        "            for start, stop in zip(start_idx, end_idx)]\n",
        "values = np.abs(partial_correlations[non_zero])\n",
        "lc = LineCollection(segments,\n",
        "                    zorder=0, cmap=plt.cm.hot_r,\n",
        "                    norm=plt.Normalize(0, .7 * values.max()))\n",
        "lc.set_array(values)\n",
        "lc.set_linewidths(15 * values)\n",
        "ax.add_collection(lc)\n",
        "\n",
        "# Add a label to each node. The challenge here is that we want to\n",
        "# position the labels to avoid overlap with other labels\n",
        "for index, (name, label, (x, y)) in enumerate(\n",
        "        zip(names, labels, embedding.T)):\n",
        "\n",
        "    dx = x - embedding[0]\n",
        "    dx[index] = 1\n",
        "    dy = y - embedding[1]\n",
        "    dy[index] = 1\n",
        "    this_dx = dx[np.argmin(np.abs(dy))]\n",
        "    this_dy = dy[np.argmin(np.abs(dx))]\n",
        "    if this_dx > 0:\n",
        "        horizontalalignment = 'left'\n",
        "        x = x + .002\n",
        "    else:\n",
        "        horizontalalignment = 'right'\n",
        "        x = x - .002\n",
        "    if this_dy > 0:\n",
        "        verticalalignment = 'bottom'\n",
        "        y = y + .002\n",
        "    else:\n",
        "        verticalalignment = 'top'\n",
        "        y = y - .002\n",
        "    plt.text(x, y, name, size=10,\n",
        "             horizontalalignment=horizontalalignment,\n",
        "             verticalalignment=verticalalignment,\n",
        "             bbox=dict(facecolor='w',\n",
        "                       edgecolor=plt.cm.nipy_spectral(label / float(n_labels)),\n",
        "                       alpha=.6))\n",
        "\n",
        "plt.xlim(embedding[0].min() - .15 * embedding[0].ptp(),\n",
        "         embedding[0].max() + .10 * embedding[0].ptp(),)\n",
        "plt.ylim(embedding[1].min() - .03 * embedding[1].ptp(),\n",
        "         embedding[1].max() + .03 * embedding[1].ptp())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEOeth_FrY8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "symbols, names = np.array(sorted(symbol_dict.items())).T\n",
        "names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30XCbsFDrYPR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooYEHHckqUPE",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}